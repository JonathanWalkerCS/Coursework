{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWw58b9dKW5r"
      },
      "source": [
        "<h2 align=\"center\"> Sentiment Analysis in Banking and Financial Services<h2>\n",
        "<h3 align=\"center\"> Natural Language Processing <h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPGhZbxeQN7S"
      },
      "outputs": [],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z21H9Mm_KW5t"
      },
      "outputs": [],
      "source": [
        "#Import required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Libraies to clean the data\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#library for data Visualization\n",
        "import wordcloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#library and function for tokanization and vectorization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#from keras_nlp.tokenizers import TextVectorization\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Importing libraries to build our models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-W3y_dUKW5v"
      },
      "source": [
        "<h3 align='center'>1. Corpus <h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBzHqLOjKW5v"
      },
      "outputs": [],
      "source": [
        "#Import our datset into our Jupyter Notebook\n",
        "\n",
        "data = pd.read_csv(\"all-data.csv\", encoding='iso-8859-1')\n",
        "#See the data\n",
        "#pd.read_csv(\"all-data.csv\", encoding='iso-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww2AMkRaKW5w"
      },
      "outputs": [],
      "source": [
        "#Print how our dataset looks like: The dataset has two columns, which are sentiments and the news.\n",
        "#Shows the first eight rows of data using the .head(value) function\n",
        "data.head(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXUfgs6tKW5w"
      },
      "outputs": [],
      "source": [
        "#rename the columns of your dataframe\n",
        "data.columns = [\"sentiment\",\"news\"]\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqqZVS6SKW5x"
      },
      "outputs": [],
      "source": [
        "#Printing the first 5 news of our dataset\n",
        "#iloc is index location, in this case a range 0 to 5 in order to print the first 5 rows/elements\n",
        "for news in data['news'].iloc[0:5]:\n",
        "    print(\"\\n\",news)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcWUe3tWKW5x"
      },
      "source": [
        "<h3 align=\"center\"> 2. Cleaning and Segmentation <h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxwpoA8UKW5x"
      },
      "outputs": [],
      "source": [
        "#Turn all Uppercase words into lowercase and remove special character and digits.\n",
        "\n",
        "#1. Turn all UpperCase and Capitalized word into lowecase.\n",
        "#implementing the lambda function\n",
        "#turning every uppercase word to lowercase\n",
        "data['news'] = data['news'].apply(lambda word: ' '.join(word.lower() for word in word.split()))\n",
        "\n",
        "#2. Remove any digits/numbers and special character\n",
        "#regular expression to identify any digits and replace them with an empty space\n",
        "data['news'] = data['news'].str.replace('\\d+',' ',regex=True)\n",
        "\n",
        "#3. Remove special characters\n",
        "data['news'] = data['news'].str.replace('\\W',' ',regex=True)\n",
        "\n",
        "#print the dataset after the clean.\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download the nltk package needed for stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "FzQ-4iv9AM8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03_GxRfoKW5y"
      },
      "outputs": [],
      "source": [
        "#Remove StopWords: To remove stopword you use the Library nltk, in particular you will need to import nltk.corpus.\n",
        "#stopwords package from the ntlk libary corpus\n",
        "#here we import the stopwords in english\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "#overriding the news and taking out the stopwords\n",
        "#split the sentence to identify the stopwords.\n",
        "#If the word is not in the stopword list, we add it\n",
        "data['news'] = data['news'].apply(lambda word:' '.join([word for word in word.split() if word not in stopwords_list]))\n",
        "\n",
        "#printing dataset\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqF8OnHBKW5y"
      },
      "outputs": [],
      "source": [
        "#Printing the first 5 news of our dataset\n",
        "\n",
        "for news in data['news'].iloc[0:5]:\n",
        "    print(\"\\n\",news)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIysBneVKW5y"
      },
      "outputs": [],
      "source": [
        "#Create a cloud of words upon their frequency: We will use two libraries wordcloud and matplotlib.\n",
        "#Largest word has the most frequency in the text\n",
        "common_words=''\n",
        "\n",
        "for i in data.news:\n",
        "    i = str(i)\n",
        "    word = i.split()\n",
        "    common_words += \" \".join(word)+\" \"\n",
        "print(common_words)\n",
        "\n",
        "\n",
        "wordcloud = wordcloud.WordCloud(width = 800, height = 800, background_color='white', min_font_size=10, collocations=False).generate(common_words)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7l7GEUTJBxwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ixoe4NsKW5z"
      },
      "source": [
        "<h3 align=\"center\">3-4. Tokanization and Vectorization<h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp2skK6FKW5z"
      },
      "outputs": [],
      "source": [
        "#Let breakdown each news into words: We will need to import Tokanizer library and keras.\n",
        "#Turning each sentence into vectors\n",
        "#Had to use add a constructor: tokenizer = Tokenizer()\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['news'].values)\n",
        "X = tokenizer.texts_to_sequences(data['news'].values)\n",
        "\n",
        "#print the news tokes-vectors.\n",
        "print(\"number of rows:{}\\nnumber of columns: {}\".format(len(X),len(X[0])))\n",
        "X[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q4Q-YxMKW5z"
      },
      "outputs": [],
      "source": [
        "#The length of our tokenized data are not even, lets set each text with the same length.\n",
        "X = pad_sequences(X)\n",
        "\n",
        "#print the news tokes-vectors.\n",
        "print(\"number of rows:{}\\nnumber of columns: {}\".format(len(X),len(X[0])))\n",
        "X[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1nxYXikKW50"
      },
      "source": [
        "<h1 align=center>5-6. Embedding and Building our Model<h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "nHUZtzWkKW50"
      },
      "outputs": [],
      "source": [
        "# Defining a new variable to store the sentiment [labels]\n",
        "Y = data['sentiment']\n",
        "\n",
        "#Using LabelEncoding:this method will turn the sentiment values into numerical values [encode strings values into numerical values]\n",
        "Y = LabelEncoder().fit_transform(Y)\n",
        "\n",
        "#Turning labels into categorical values.\n",
        "Y = to_categorical(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XFeHHgh9KW50",
        "outputId": "22fdf812-93df-427a-8e1f-9740c308f30d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples for training [news]:(3876, 41)\n",
            "Number of sample for training [Labels]:(3876, 3)\n",
            "Number of samples for testing [news]:(969, 41)\n",
            "Number of sample for testing [Labels]:(969, 3)\n"
          ]
        }
      ],
      "source": [
        "#Splitting the data into training and testing\n",
        "#X_train contains the news used to train the mode, Y_train is used to test it\n",
        "#Y is the categorical labels\n",
        "#train will contain 80% of the news\n",
        "#test will contain 20% of the news\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20)\n",
        "\n",
        "print(\"Number of samples for training [news]:{}\\nNumber of sample for training [Labels]:{}\".format(X_train.shape,y_train.shape))\n",
        "print(\"Number of samples for testing [news]:{}\\nNumber of sample for testing [Labels]:{}\".format(X_test.shape,y_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVcA_Q4aKW50"
      },
      "outputs": [],
      "source": [
        "#define model\n",
        "#Input shape: the number of features\n",
        "#Pad sequence: defines the length of your array (number of features)\n",
        "#Flatten turns 4x4 layers into 1x16 for example\n",
        "#Dense(3,) is 3 layers to represent the categorization\n",
        "n_features = 31\n",
        "model = Sequential()\n",
        "model.add(Embedding(500,120,input_shape=(n_features,)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu', kernel_initializer ='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer ='he_normal'))\n",
        "model.add(Dense(3, activation='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Jt67gykFKW50",
        "outputId": "d5b7b3b1-f2d5-4b4c-ead0-1418c2db2964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 3720, but received input with shape (None, 4920)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 41), dtype=int32)\n  • training=True\n  • mask=None",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-1a5a3ff94e49>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 }:\n\u001b[0;32m--> 227\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    228\u001b[0m                         \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;34mf\"incompatible with the layer: expected axis {axis} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 3720, but received input with shape (None, 4920)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(None, 41), dtype=int32)\n  • training=True\n  • mask=None"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "#can name the model anything, we just used the word model here\n",
        "#crossentropy is a function used for classification tasks\n",
        "#metric is used to check the accuracy\n",
        "#Still WORKING on this section\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# fit the model\n",
        "model.fit(X_train,y_train,epochs=20, batch_size=32, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLt_EAIGKW50"
      },
      "outputs": [],
      "source": [
        "# Evaluate out model using the accuracy metric\n",
        "loss, acc = model.evaluate( )\n",
        "print('Test Accuracy: %.3f' % acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YNQWUH5KW51"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model\n",
        "sentiment_prediction = model.predict(X_test)\n",
        "predicted_inverse = np.argmax(sentiment_prediction,axis=1)\n",
        "y_test_inverse = np.argmax(y_test,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSyQFuwKKW51"
      },
      "outputs": [],
      "source": [
        "target_name = [\"Class {}\".format(i) for i in range(3)]\n",
        "\n",
        "print( )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}