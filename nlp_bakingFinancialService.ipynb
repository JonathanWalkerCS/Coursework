{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonathanWalkerCS/Coursework/blob/main/nlp_bakingFinancialService.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWw58b9dKW5r"
      },
      "source": [
        "<h2 align=\"center\"> Sentiment Analysis in Banking and Financial Services<h2>\n",
        "<h3 align=\"center\"> Natural Language Processing <h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPGhZbxeQN7S"
      },
      "outputs": [],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z21H9Mm_KW5t"
      },
      "outputs": [],
      "source": [
        "#Import required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Libraies to clean the data\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#library for data Visualization\n",
        "import wordcloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#library and function for tokanization and vectorization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#from keras_nlp.tokenizers import TextVectorization\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Importing libraries to build our models\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-W3y_dUKW5v"
      },
      "source": [
        "<h3 align='center'>1. Corpus <h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBzHqLOjKW5v"
      },
      "outputs": [],
      "source": [
        "#Import our datset into our Jupyter Notebook\n",
        "\n",
        "data = pd.read_csv(\"all-data.csv\", encoding='iso-8859-1')\n",
        "#See the data\n",
        "#pd.read_csv(\"all-data.csv\", encoding='iso-8859-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww2AMkRaKW5w"
      },
      "outputs": [],
      "source": [
        "#Print how our dataset looks like: The dataset has two columns, which are sentiments and the news.\n",
        "#Shows the first eight rows of data using the .head(value) function\n",
        "data.head(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXUfgs6tKW5w"
      },
      "outputs": [],
      "source": [
        "#rename the columns of your dataframe\n",
        "data.columns = [\"sentiment\",\"news\"]\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqqZVS6SKW5x"
      },
      "outputs": [],
      "source": [
        "#Printing the first 5 news of our dataset\n",
        "#iloc is index location, in this case a range 0 to 5 in order to print the first 5 rows/elements\n",
        "for news in data['news'].iloc[0:5]:\n",
        "    print(\"\\n\",news)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcWUe3tWKW5x"
      },
      "source": [
        "<h3 align=\"center\"> 2. Cleaning and Segmentation <h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxwpoA8UKW5x"
      },
      "outputs": [],
      "source": [
        "#Turn all Uppercase words into lowercase and remove special character and digits.\n",
        "\n",
        "#1. Turn all UpperCase and Capitalized word into lowecase.\n",
        "#implementing the lambda function\n",
        "#turning every uppercase word to lowercase\n",
        "data['news'] = data['news'].apply(lambda word: ' '.join(word.lower() for word in word.split()))\n",
        "\n",
        "#2. Remove any digits/numbers and special character\n",
        "#regular expression to identify any digits and replace them with an empty space\n",
        "data['news'] = data['news'].str.replace('\\d+',' ',regex=True)\n",
        "\n",
        "#3. Remove special characters\n",
        "data['news'] = data['news'].str.replace('\\W',' ',regex=True)\n",
        "\n",
        "#print the dataset after the clean.\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download the nltk package needed for stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "FzQ-4iv9AM8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03_GxRfoKW5y"
      },
      "outputs": [],
      "source": [
        "#Remove StopWords: To remove stopword you use the Library nltk, in particular you will need to import nltk.corpus.\n",
        "#stopwords package from the ntlk libary corpus\n",
        "#here we import the stopwords in english\n",
        "stopwords_list = set(stopwords.words('english'))\n",
        "\n",
        "#overriding the news and taking out the stopwords\n",
        "#split the sentence to identify the stopwords.\n",
        "#If the word is not in the stopword list, we add it\n",
        "data['news'] = data['news'].apply(lambda word:' '.join([word for word in word.split() if word not in stopwords_list]))\n",
        "\n",
        "#printing dataset\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqF8OnHBKW5y"
      },
      "outputs": [],
      "source": [
        "#Printing the first 5 news of our dataset\n",
        "\n",
        "for news in data['news'].iloc[0:5]:\n",
        "    print(\"\\n\",news)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIysBneVKW5y"
      },
      "outputs": [],
      "source": [
        "#Create a cloud of words upon their frequency: We will use two libraries wordcloud and matplotlib.\n",
        "#Largest word has the most frequency in the text\n",
        "common_words=''\n",
        "\n",
        "for i in data.news:\n",
        "    i = str(i)\n",
        "    word = i.split()\n",
        "    common_words += \" \".join(word)+\" \"\n",
        "print(common_words)\n",
        "\n",
        "\n",
        "wordcloud = wordcloud.WordCloud(width = 800, height = 800, background_color='white', min_font_size=10, collocations=False).generate(common_words)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7l7GEUTJBxwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ixoe4NsKW5z"
      },
      "source": [
        "<h3 align=\"center\">3-4. Tokanization and Vectorization<h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp2skK6FKW5z"
      },
      "outputs": [],
      "source": [
        "#Let breakdown each news into words: We will need to import Tokanizer library and keras.\n",
        "#Turning each sentence into vectors\n",
        "#Had to use add a constructor: tokenizer = Tokenizer()\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data['news'].values)\n",
        "X = tokenizer.texts_to_sequences(data['news'].values)\n",
        "\n",
        "#print the news tokes-vectors.\n",
        "print(\"number of rows:{}\\nnumber of columns: {}\".format(len(X),len(X[0])))\n",
        "X[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q4Q-YxMKW5z"
      },
      "outputs": [],
      "source": [
        "#The length of our tokenized data are not even, lets set each text with the same length.\n",
        "X = pad_sequences(X)\n",
        "\n",
        "#print the news tokes-vectors.\n",
        "print(\"number of rows:{}\\nnumber of columns: {}\".format(len(X),len(X[0])))\n",
        "X[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1nxYXikKW50"
      },
      "source": [
        "<h1 align=center>5-6. Embedding and Building our Model<h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHUZtzWkKW50"
      },
      "outputs": [],
      "source": [
        "# Defining a new variable to store the sentiment [labels]\n",
        "Y = data['sentiment']\n",
        "\n",
        "#Using LabelEncoding:this method will turn the sentiment values into numerical values [encode strings values into numerical values]\n",
        "Y = LabelEncoder().fit_transform(Y)\n",
        "\n",
        "#Turning labels into categorical values.\n",
        "Y = to_categorical(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFeHHgh9KW50"
      },
      "outputs": [],
      "source": [
        "#Splitting the data into training and testing\n",
        "#X_train contains the news used to train the mode, Y_train is used to test it\n",
        "#Y is the categorical labels\n",
        "#train will contain 80% of the news\n",
        "#test will contain 20% of the news\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20)\n",
        "\n",
        "print(\"Number of samples for training [news]:{}\\nNumber of sample for training [Labels]:{}\".format(X_train.shape,y_train.shape))\n",
        "print(\"Number of samples for testing [news]:{}\\nNumber of sample for testing [Labels]:{}\".format(X_test.shape,y_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVcA_Q4aKW50"
      },
      "outputs": [],
      "source": [
        "#define model\n",
        "#Input shape: the number of features\n",
        "#Pad sequence: defines the length of your array (number of features)\n",
        "#Flatten turns 4x4 layers into 1x16 for example\n",
        "#Dense(3,) is 3 layers to represent the categorization\n",
        "n_features = 31\n",
        "model = Sequential()\n",
        "model.add(Embedding(500,120,input_shape=(n_features,)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='relu', kernel_initializer ='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(8, activation='relu', kernel_initializer ='he_normal'))\n",
        "model.add(Dense(3, activation='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt67gykFKW50"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "#can name the model anything, we just used the word model here\n",
        "#crossentropy is a function used for classification tasks\n",
        "#metric is used to check the accuracy\n",
        "#Still WORKING on this section\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# fit the model\n",
        "model.fit(X_train,y_train,epochs=20, batch_size=32, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLt_EAIGKW50"
      },
      "outputs": [],
      "source": [
        "# Evaluate out model using the accuracy metric\n",
        "loss, acc = model.evaluate( )\n",
        "print('Test Accuracy: %.3f' % acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YNQWUH5KW51"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model\n",
        "sentiment_prediction = model.predict(X_test)\n",
        "predicted_inverse = np.argmax(sentiment_prediction,axis=1)\n",
        "y_test_inverse = np.argmax(y_test,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSyQFuwKKW51"
      },
      "outputs": [],
      "source": [
        "target_name = [\"Class {}\".format(i) for i in range(3)]\n",
        "\n",
        "print( )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}